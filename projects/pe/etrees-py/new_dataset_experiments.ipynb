{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import time\n",
    "# from catboost import CatBoostRegressor, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training data set\n",
    "training_data_path = \"../data/08_3_2022/data3/a_extract_regression_data_1n1p_tb.txt\"\n",
    "# Path to testing data set\n",
    "testing_data_path = \"../data/08_3_2022/data3/c_extract_regression_data_1n1p_hb.txt\"\n",
    "# Whether to normalize data or not\n",
    "norm = True\n",
    "# Name of the model\n",
    "m_name = \"ert\"\n",
    "# Whether to load an existing model\n",
    "load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_ert():\n",
    "    return ExtraTreesRegressor(n_estimators=100, n_jobs=-1, verbose=True)\n",
    "    # return CatBoostRegressor(objective='MultiRMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_n_and_load_dataset(n,path):\n",
    "    accumulator = []\n",
    "    with open(path,'r') as f:\n",
    "        for _ in range(n):\n",
    "            next(f)\n",
    "        for line in f:\n",
    "            data = line.split(\":\")\n",
    "            first_part = data[0].lstrip().split()\n",
    "            second_part = data[1].lstrip().split()\n",
    "            third_part = data[2].split(\"==>\")[1].lstrip().split()\n",
    "            extra_value = data[2].split(\"==>\")[0].lstrip().split()[2]\n",
    "            sec = first_part[0]\n",
    "            charge = first_part[1]\n",
    "            output = second_part\n",
    "            input = third_part[:6]\n",
    "            # accumulator.append([int(sec), int(charge), float(input[0]), float(input[1]), float(input[2]), float(input[3]), float(input[4]), float(input[5]), float(output[0]), float(output[1]), float(output[2]) , float(extra_value)])\n",
    "            accumulator.append([int(sec), int(charge), float(input[0]), float(input[1]), float(input[2]), float(input[3]), float(input[4]), float(input[5]), float(output[0]), float(output[1]), float(output[2])])\n",
    "            # print(line)\n",
    "    return np.array(accumulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = skip_n_and_load_dataset(0,training_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector\tCharge\tIn[0]\tIn[1]\tIn[2]\tIn[3]\tIn[4]\tIn[5]\tOut[0]\tOut[1]\tOut[2]\n",
      "[[   4.       -1.       18.571    17.571    18.833    17.857    23.833\n",
      "    22.833     4.9017   10.3041  172.0683]\n",
      " [   3.        1.       46.667    43.       40.667    35.       23.\n",
      "    18.        1.0985   20.7015   70.3843]\n",
      " [   3.       -1.       20.5      21.333    21.       21.833    26.4\n",
      "    27.167     4.0101   11.0097  127.0826]\n",
      " [   1.        1.       60.5      64.5      56.       58.833    48.143\n",
      "    50.167     2.0701   25.3714   -1.392 ]\n",
      " [   4.       -1.        4.833     5.333     8.167    10.       24.167\n",
      "    26.        1.9716    6.3195 -169.1469]]\n"
     ]
    }
   ],
   "source": [
    "# Validate that the data look fine. Data layout:\n",
    "print(\"Sector\\tCharge\\tIn[0]\\tIn[1]\\tIn[2]\\tIn[3]\\tIn[4]\\tIn[5]\\tOut[0]\\tOut[1]\\tOut[2]\")  \n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7316, 3)\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets based on charge and filter sector 2\n",
    "\n",
    "# Sector 2 data\n",
    "sec_2_filter = data[:, 0] == 2\n",
    "sec_2_data = data[sec_2_filter]\n",
    "\n",
    "# Sector 2 pos charge\n",
    "sec_2_pos_charge_filter = sec_2_data[:, 1] == 1\n",
    "sec_2_pos_data = sec_2_data[sec_2_pos_charge_filter]\n",
    "\n",
    "# Sector 2 neg charge\n",
    "sec_2_neg_charge_filter = sec_2_data[:, 1] == -1\n",
    "sec_2_neg_data = sec_2_data[sec_2_neg_charge_filter]\n",
    "\n",
    "# Normalize X data\n",
    "X_data_pos = sec_2_pos_data[:,2:8]/111\n",
    "X_data_neg = sec_2_neg_data[:,2:8]/111\n",
    "\n",
    "# Prepar y data\n",
    "y_data_pos = sec_2_pos_data[:,8:]\n",
    "y_data_neg = sec_2_neg_data[:,8:]\n",
    "print(y_data_pos.shape)\n",
    "# print(all_pos_data[:5])\n",
    "# print(all_neg_data[:5])\n",
    "# print(sec_2_data.shape)\n",
    "# print(sec_2_pos_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of 100 | elapsed:    1.4s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=80)]: Using backend ThreadingBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=80)]: Done  42 out of 100 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=80)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score neg:  0.9999992622616881\n"
     ]
    }
   ],
   "source": [
    "# If training a new model prepare the scaler and train the network\n",
    "# Otherwise load existing network and scaler\n",
    "\n",
    "if load == False:\n",
    "    clf_neg = create_ert()\n",
    "    X_scaler_neg = StandardScaler()\n",
    "    y_scaler_neg = StandardScaler()\n",
    "    X_train_neg, X_test_neg, y_train_neg, y_test_neg =  train_test_split(X_data_neg, y_data_neg, test_size=0.05, random_state = 42)\n",
    "    y_train_neg_norm = y_scaler_neg.fit_transform(y_train_neg)\n",
    "    if norm:\n",
    "        import pickle\n",
    "        y_test_neg_norm = y_scaler_neg.transform(y_test_neg)\n",
    "        clf_neg.fit(X_train_neg, y_train_neg_norm)\n",
    "        with open(m_name+'_neg.model', 'wb') as file:\n",
    "            pickle.dump(clf_neg, file)\n",
    "        with open(m_name+'_neg.scaler', 'wb') as file:\n",
    "            pickle.dump(y_train_neg_norm, file)\n",
    "        score_neg = clf_neg.score(X_train_neg, y_train_neg_norm)\n",
    "        print(\"Training score neg: \",score_neg)\n",
    "\n",
    "    else:\n",
    "        clf_neg.fit(X_train_neg, y_train_neg)\n",
    "        score_neg = clf_neg.score(X_train_neg, y_train_neg)\n",
    "        print(\"Training score neg: \",score_neg)\n",
    "else:\n",
    "    import pickle\n",
    "    with open(m_name+'_neg.model', 'rb') as file:\n",
    "        clf_neg = pickle.load(file)\n",
    "    if norm:\n",
    "        with open(m_name+'_neg.scaler', 'rb') as file:\n",
    "            y_scaler_neg = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of 100 | elapsed:    1.6s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=80)]: Using backend ThreadingBackend with 80 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score pos:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=80)]: Done  42 out of 100 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=80)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# If training a new model prepare the scaler and train the network\n",
    "# Otherwise load existing network and scaler\n",
    "\n",
    "if load == False:\n",
    "    clf_pos = create_ert()\n",
    "    X_scaler_pos = StandardScaler()\n",
    "    y_scaler_pos = StandardScaler()\n",
    "    X_train_pos, X_test_pos, y_train_pos, y_test_pos =  train_test_split(X_data_pos, y_data_pos, test_size=0.05, random_state = 42)\n",
    "    y_train_pos_norm = y_scaler_pos.fit_transform(y_train_pos)\n",
    "    if norm:\n",
    "        import pickle\n",
    "        y_test_pos_norm = y_scaler_pos.transform(y_test_pos)\n",
    "        clf_pos.fit(X_train_pos, y_train_pos_norm)\n",
    "        with open(m_name+'_pos.model', 'wb') as file:\n",
    "            pickle.dump(clf_pos, file)\n",
    "        with open(m_name+'_pos.scaler', 'wb') as file:\n",
    "            pickle.dump(y_train_pos_norm, file)\n",
    "        score_pos = clf_pos.score(X_train_pos, y_train_pos_norm)\n",
    "        print(\"Training score pos: \",score_pos)\n",
    "\n",
    "    else:\n",
    "        clf_pos.fit(X_train_pos, y_train_pos)\n",
    "        score_pos = clf_pos.score(X_train_pos, y_train_pos)\n",
    "        print(\"Training score pos: \",score_pos)\n",
    "else:\n",
    "    import pickle\n",
    "    with open(m_name+'_pos.model', 'rb') as file:\n",
    "        clf_pos = pickle.load(file)\n",
    "    if norm:\n",
    "        with open(m_name+'_pos.scaler', 'rb') as file:\n",
    "            y_scaler_pos = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=80)]: Using backend ThreadingBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=80)]: Done  42 out of 100 | elapsed:    0.2s remaining:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score_pos:  0.9776301273908015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=80)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on sector 2 validation set (pos)\n",
    "\n",
    "if norm:\n",
    "    score_pos = clf_pos.score(X_test_pos, y_test_pos_norm)\n",
    "    print(\"Testing score_pos: \",score_pos)\n",
    "else :\n",
    "    score_pos = clf_pos.score(X_test_pos, y_test_pos)\n",
    "    print(\"Testing score_pos: \",score_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score_neg:  0.9882631265895924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=80)]: Using backend ThreadingBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=80)]: Done  42 out of 100 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=80)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on sector 2 validation set (neg)\n",
    "\n",
    "if norm:\n",
    "    score_neg = clf_neg.score(X_test_neg, y_test_neg_norm)\n",
    "    print(\"Testing score_neg: \",score_neg)\n",
    "else :\n",
    "    score_neg = clf_neg.score(X_test_neg, y_test_neg)\n",
    "    print(\"Testing score_neg: \",score_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing data\n",
    "\n",
    "data_read = skip_n_and_load_dataset(0, testing_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare testing data set based on charge and normalize with scaler\n",
    "\n",
    "data = data_read\n",
    "# sec_3_data = data[data[:, 0] == 3] \n",
    "# sec_2_data = data[data[:, 0] == 2] \n",
    "# sec_3_data[:, 10] -= 60\n",
    "# data = sec_3_data\n",
    "# data = np.concatenate((sec_3_data, sec_2_data))\n",
    "all_pos_data_filter = data[:,1] == 1\n",
    "all_neg_data_filter = data[:,1] == -1\n",
    "all_pos_data = data[all_pos_data_filter]\n",
    "all_neg_data = data[all_neg_data_filter]\n",
    "\n",
    "X_all_pos = all_pos_data[:,2:8]/111\n",
    "y_all_pos = all_pos_data[:, 8:]\n",
    "y_all_pos_norm = y_scaler_pos.transform(y_all_pos)\n",
    "\n",
    "X_all_neg = all_neg_data[:,2:8]/111\n",
    "y_all_neg = all_neg_data[:, 8:]\n",
    "\n",
    "y_all_neg_norm = y_scaler_neg.transform(y_all_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67010, 6)\n",
      "(67010, 3)\n",
      "[[  5.      -1.      30.286   34.167   30.      34.      34.8     38.857\n",
      "    2.7493  15.2538 -89.4344]\n",
      " [  3.       1.      74.5     75.      64.167   62.5     44.833   42.4\n",
      "    0.7025  35.0741  71.5885]\n",
      " [  3.      -1.      13.667   15.      14.167   15.571   19.833   21.167\n",
      "    5.0194   8.9759 130.2519]\n",
      " [  1.       1.      53.      54.167   44.857   43.143   22.5     19.444\n",
      "    0.8124  24.512  -34.6423]\n",
      " [  3.      -1.      20.571   21.333   21.143   21.833   26.4     27.167\n",
      "    4.063   11.4135 123.5638]]\n",
      "[[  3.       1.      74.5     75.      64.167   62.5     44.833   42.4\n",
      "    0.7025  35.0741  71.5885]\n",
      " [  1.       1.      53.      54.167   44.857   43.143   22.5     19.444\n",
      "    0.8124  24.512  -34.6423]\n",
      " [  1.       1.      60.5     64.5     56.      58.833   48.143   50.167\n",
      "    2.104   26.502    0.5452]\n",
      " [  3.       1.      60.143   67.      52.667   55.857   30.      29.375\n",
      "    0.6113  33.2836  90.7207]\n",
      " [  4.       1.      58.167   64.      52.286   55.5     37.857   39.429\n",
      "    0.9769  27.4319 167.7541]]\n"
     ]
    }
   ],
   "source": [
    "print(X_all_pos.shape)\n",
    "print(y_all_pos.shape)\n",
    "print(data[:5])\n",
    "print(all_pos_data[:5])\n",
    "# pos_predict = clf_pos.predict(X_all_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=80)]: Using backend ThreadingBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=80)]: Done  42 out of 100 | elapsed:    1.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=80)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=80)]: Using backend ThreadingBackend with 80 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Positive Data:  0.6161449644252611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=80)]: Done  42 out of 100 | elapsed:    1.8s remaining:    2.5s\n",
      "[Parallel(n_jobs=80)]: Done 100 out of 100 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on all sectors of testing data set (pos)\n",
    "\n",
    "pos_predict = None\n",
    "if norm:\n",
    "    score_pos = clf_pos.score(X_all_pos, y_all_pos_norm)\n",
    "    print(\"Score Positive Data: \", score_pos)\n",
    "    pos_predict = y_scaler_pos.inverse_transform(clf_pos.predict(X_all_pos))\n",
    "else:\n",
    "    score_pos = clf_pos.score(X_all_pos, y_all_pos)\n",
    "    print(\"Score Positive Data: \", score_pos)\n",
    "    pos_predict = clf_pos.predict(X_all_pos)\n",
    "# print(pos_predict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=80)]: Using backend ThreadingBackend with 80 concurrent workers.\n",
      "[Parallel(n_jobs=80)]: Done  42 out of 100 | elapsed:    1.5s remaining:    2.0s\n",
      "[Parallel(n_jobs=80)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "/home/pthom001/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"\n",
      "[Parallel(n_jobs=80)]: Using backend ThreadingBackend with 80 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score negitive Data:  0.47250186188511695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=80)]: Done  42 out of 100 | elapsed:    1.8s remaining:    2.5s\n",
      "[Parallel(n_jobs=80)]: Done 100 out of 100 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 3.9883179999999996\n",
      "[ 5.129381  8.949817 71.814066]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthom001/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on all sectors of testing data set (neg)\n",
    "\n",
    "neg_predict = None\n",
    "if norm:\n",
    "    score_neg = clf_neg.score(X_all_neg, y_all_neg_norm)\n",
    "    print(\"Score negitive Data: \", score_neg)\n",
    "    start = time.clock()\n",
    "    neg_predict = y_scaler_neg.inverse_transform(clf_neg.predict(X_all_neg))\n",
    "    end = time.clock()\n",
    "else:\n",
    "    score_neg = clf_neg.score(X_all_neg, y_all_neg)\n",
    "    print(\"Score Negative Data: \", score_neg)\n",
    "    start = time.clock()\n",
    "    neg_predict = clf_neg.predict(X_all_neg)\n",
    "    end = time.clock()\n",
    "print(\"Time {}\".format(end-start))\n",
    "print(neg_predict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends predictions to testing data set\n",
    "\n",
    "def skip_n_and_append_predictions_to_file(n, path):\n",
    "    index_pos = 0\n",
    "    index_neg = 0\n",
    "    with open(path, 'r') as f:\n",
    "        suffix = '.pred'\n",
    "        if norm :\n",
    "            suffix += '.norm'\n",
    "        with open(path+suffix, 'w') as out:\n",
    "            for _ in range(n):\n",
    "                out.write(f.readline())\n",
    "            for line in f:\n",
    "                data = line.split(\":\")\n",
    "                first_part = data[0].lstrip().split()\n",
    "                second_part = data[1].lstrip().split()\n",
    "                third_part = data[2].split(\"==>\")[1].lstrip().split()\n",
    "                extra_value = data[2].split(\"==>\")[0].lstrip().split()[2]\n",
    "                sec = first_part[0]\n",
    "                charge = first_part[1]\n",
    "                output = second_part\n",
    "                input = third_part[:6]\n",
    "                if int(charge) == -1:\n",
    "                    if y_all_neg[index_neg, 0]  == float(output[0]) and y_all_neg[index_neg, 1]  == float(output[1]) and y_all_neg[index_neg, 2]  == float(output[2]):\n",
    "                    # if y_all_neg[index_neg, 0]  == float(extra_value):\n",
    "                        out.write(line.strip() + \"   {} {} {}\\n\".format(neg_predict[index_neg, 0], neg_predict[index_neg, 1], neg_predict[index_neg, 2]))\n",
    "                        # out.write(line.strip() + \": {} \\n\".format(neg_predict[index_pos]))\n",
    "                    else:\n",
    "                        print(line)\n",
    "                        print(\"ERROR!\")\n",
    "                        break\n",
    "                    index_neg += 1\n",
    "                elif int(charge) == 1:\n",
    "                    if y_all_pos[index_pos, 0]  == float(output[0]) and y_all_pos[index_pos, 1]  == float(output[1]) and y_all_pos[index_pos, 2]  == float(output[2]):\n",
    "                    # if y_all_pos[index_pos, 0]  == float(extra_value):\n",
    "                        out.write(line.strip() + \"   {} {} {}\\n\".format(pos_predict[index_pos, 0], pos_predict[index_pos, 1], pos_predict[index_pos, 2]))\n",
    "                        # out.write(line.strip() + \": {} \\n\".format(pos_predict[index_pos]))\n",
    "                    else:\n",
    "                        print(\"ERROR!\")\n",
    "                        break\n",
    "                    index_pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append predictions to testing set\n",
    "skip_n_and_append_predictions_to_file(0, testing_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586cb8c90e2feff4d88f5961586ac0d131f5b2b91c8ef51c7f785394dc7da771"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
